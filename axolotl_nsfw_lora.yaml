# Base model downloaded locally (via snapshot_download)
base_model: ./llama4_scout_17b
model_type: llama
tokenizer_type: llama
load_in_4bit: true

special_tokens:
  pad_token: "<unk>"

datasets:
  - path: ./nsfw_combined_dataset_tagged.jsonl
    type: alpaca

adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

sequence_len: 2048
cutoff_len: 2048
train_on_inputs: false
add_eos_token: true

micro_batch_size: 1
gradient_accumulation_steps: 4
num_epochs: 3
learning_rate: 5e-5

optimizer: adamw_torch
lr_scheduler: cosine
warmup_steps: 10
gradient_checkpointing: true

output_dir: ./outputs
save_steps: 100
logging_steps: 10

bnb_4bit_compute_dtype: float16
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
